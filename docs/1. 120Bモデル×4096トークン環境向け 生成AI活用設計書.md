# **120Bモデル×4096トークン環境向け 生成AI活用MVP設計書**

**作成日:** 2026年2月

**対象環境:** 社内オンプレミス / 専用GPUマシン

**稼働モデル:** GPT-oss-120b (想定仕様: 120Bパラメータ, 最大コンテキスト長4096トークン, 推論回数・コスト無制限)

## **1\. 背景とアーキテクチャの基本思想**

本設計書は、パラメータ数が120Bクラスという極めて高い推論能力を持ちながら、入力コンテキストが4096トークン（日本語でおそらく2000〜3000文字程度、コードで数百行程度）に制限されている生成AI環境を、ソフトウェア開発において最大限に活用するためのアプローチを定義する。

### **1.1. 環境の特性分析**

* **強み:** 120Bクラスのモデルは、高度な論理推論、複雑な要件の理解、多角的なコードレビューにおいて、人間と同等以上の洞察力を発揮する。また、社内環境のためAPIコストを気にせず24時間無制限に推論を回し続けることができる。  
* **弱み:** 4096トークンの制約により、ファイル間の依存関係やシステム全体のアーキテクチャを一度に読み込ませることは不可能である。「おもちゃの懐中電灯で巨大な工場を見学する」状態に等しい。

### **1.2. コア戦略：全体把握の放棄と「役割（ウェア）」の多重化**

コンテキスト制限の壁を突破するため、AIに「システム全体の把握」をさせることを設計上完全に放棄する。

代わりに、\*\*「コードを極小の単位に分割（チャンク化）」**し、**「1つのコード片に対して、AIの役割（ウェア）を切り替えながら何度も推論を重ねる」\*\*非同期のバッチ処理アプローチを採用する。

## **2\. 具体的な活用ユースケース**

推論コストゼロの利点を活かした、3つの具体的な活用パイプラインを提案する。

### **ユースケースA: バックグラウンド多重マイクロ監査（Micro-Auditing）**

人間が書いたコードに対し、異なる専門家の視点（ウェア）を持たせたAIエージェントを複数稼働させ、局所的な品質を極限まで高める。

* **処理単位:** 1つの関数、または1つのクラス（数行〜数百行）。  
* **ウェア（役割）の例:**  
  1. **セキュリティ専門家:** 「この数十行の中にインジェクションやメモリリークの脆弱性はないか」にのみ集中する。  
  2. **パフォーマンスチューナー:** 「O(N^2)の非効率なループや、不要なメモリアロケーションがないか」を監査する。  
  3. **命名規則・認知負荷レビュアー:** 「変数名が実態を表しているか、ネストが深すぎないか」を監査する。  
* **運用:** 開発者がファイルを保存するたびにバックグラウンドで上記3つの監査が走り、該当行に対してエディタ上で警告（Diagnostics）を非同期に出力する。

### **ユースケースB: 「設計思想（Why）」のリバースエンジニアリングと蓄積**

コードの「振る舞い（What）」ではなく、120Bモデルの推論能力を活かして「なぜそのように書かれたか（Why）」を推測させ、ドキュメントとして自動生成・蓄積する。

* **処理フロー:**  
  1. リポジトリ内の全関数を順番にAIに読み込ませる。  
  2. プロンプト：「この関数の実装から推測される、業務上の意図や、妥協した技術的制約（レガシー互換性など）を日本語で説明せよ」。  
  3. 出力されたテキストを、関数名のハッシュ値などをキーとしてローカルのベクトルデータベースに保存する。  
* **運用:** 新規参画メンバーが「この複雑な処理の意図は何か？」と疑問に思った際、蓄積されたデータベースを検索（RAG）することで、コードベース全体の設計思想を疑似的に把握できる。

### **ユースケースC: AST抽出による「スケルトン解析」**

4096トークンの制約内で広い範囲を俯瞰するための、コード圧縮アプローチ。

* **処理フロー:**  
  1. ソースコードから実装の中身（処理ブロック）をプログラム（ASTパーサー等）で削除し、「関数名」「引数・戻り値」「Docstring（コメント）」のみを残した「スケルトン（骨組み）コード」を生成する。  
  2. 数百行のコードがおそらく数十行に圧縮されるため、複数ファイル分のインターフェース情報を4096トークン内に収めることが可能になる。  
  3. これをAIに読み込ませ、「モジュール間の結合度の評価」や「全体的なリファクタリング計画の立案（プランニング）」を行わせる。

## **3\. システムアーキテクチャ案（MVP）**

開発者のエディタ体験（UX）を損なわずに上記のユースケースを実現するための、3層ハイブリッド構成。

### **3.1. 構成要素**

1. **UI層（VS Code 拡張機能）**  
   * 既存のエディタに組み込み、開発者とのインターフェースを担う。  
   * ファイルの保存イベント（OnSave）をフックし、ロジック層へパスを送信。  
   * ロジック層から受け取った指摘事項を、エディタの該当行に非同期で波線表示・ホバー表示する。  
2. **ロジック層（CLIデーモン / エージェントエンジン）**  
   * OSのバックグラウンドで常駐する独立したプロセス（Python等を想定）。  
   * **AST解析:** 対象コードを言語ごとのパーサーで関数単位にチャンク化し、4096トークン未満であることを保証する。  
   * **タスクキュー:** 解析タスクをキューに積み、AIへのプロンプト（ウェア）を切り替えながら順次リクエストを投げる。  
3. **推論層・記憶層（ローカルサーバー）**  
   * **LLM API:** GPT-oss-120b をホストするエンドポイント。  
   * **ローカルDB (SQLite / ChromaDB等):** チャンクのハッシュ値を保存し、変更のない関数の重複推論をスキップ（キャッシュ）する。また、ユースケースBで抽出した設計思想を永続化する。

### **3.2. アーキテクチャ図（概念）**

\[ Developer / VS Code \]  
       | (File Save Event / Display Diagnostics)  
       v  
\[ CLI Daemon (Background) \]  
  ├─ AST Parser (Chunking)  
  ├─ Task Queue & Wear Manager (Prompt Switching)  
  └─ Cache Manager  
       | (Async API Requests)  
       v  
\[ Local Environment \]  
  ├─ LLM: GPT-oss-120b (Max 4096 tokens)  
  └─ DB: SQLite / Vector Store

## **4\. 実装における留意点と課題**

* **トークン管理の厳格化:** 4096トークンを超過するとエラーや文脈の切断が発生するため、入力前にトークン数を計算する処理が必須となる。公式のTiktoken等のトークナイザーが当該モデルで利用できない場合は、文字数ベース（例: 日本語・英語混在で最大2000文字など）の安全なハードリミットを設ける必要がある。  
* **AST解析の言語依存:** 関数単位の正確な抽出には言語ごとの静的解析ツール（Pythonの ast、TypeScriptの TypeScript Compiler API など）が必要。MVP段階では、利用頻度の高い1つのプログラミング言語に対象を絞るアプローチが推奨される。  
* **プロンプトエンジニアリング:** 120Bモデルは指示に対する忠実性が高いため、「あなたは〇〇の専門家です。以下のコード片のみを見て、〇〇の観点からのみ指摘してください。コード全体の文脈は無視して構いません」という、スコープを意図的に狭めるシステムプロンプトの設計が精度向上に直結する。