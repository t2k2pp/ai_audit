# =============================================================================
# ai_audit 設定ファイル
# =============================================================================
# このファイルを .env にコピーして、各項目を自分の環境に合わせて編集してください。
#   cp .env.example .env
#
# ★ 必須項目（未設定のままだとツールが起動しません）
# ★ 任意項目（未設定でも動作します）
# =============================================================================


# =============================================================================
# ★ 必須: LLM API 接続先
# =============================================================================
# Ollama を使う場合: http://<サーバーIP>:<ポート>/v1
#   例: http://192.168.1.40:11434/v1
#
# OpenAI API を使う場合: https://api.openai.com/v1
#
LLM_API_BASE_URL=


# =============================================================================
# ★ 必須: API キー
# =============================================================================
# Ollama を使う場合: 任意の文字列でOK（例: ollama）
# OpenAI API を使う場合: sk-... 形式のキー
#
LLM_API_KEY=


# =============================================================================
# ★ 必須: 使用するモデル名
# =============================================================================
# 利用可能なモデルは以下で確認できます:
#   python main.py config model
#
# 例: gpt-oss:120b / llama3.1:8b / codestral:22b / gpt-4o
#
LLM_MODEL_NAME=


# =============================================================================
# 任意: 最大出力トークン数
# =============================================================================
# LLM が1回のリクエストで生成できるトークン数の上限（出力のみ。入力は別）
#
# 未設定（コメントアウト）→ モデルのデフォルト最大値を使用（推奨）
# 整数値                  → 指定値を上限とする（大きいほど詳細な指摘、推論時間は増加）
#
# CLIからも変更できます:
#   python main.py config output-tokens 4096   # 上限を指定
#   python main.py config output-tokens auto   # モデルのデフォルト最大値に戻す
#
# LLM_MAX_OUTPUT_TOKENS=4096
